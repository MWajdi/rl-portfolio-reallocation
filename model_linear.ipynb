{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples loaded: 195000\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def load_data_from_json(json_file):\n",
    "    \"\"\"\n",
    "    Loads the JSON file created by `create_train_json` and returns\n",
    "    a sorted list of (timestamp_dt, X_window, y_val).\n",
    "    \"\"\"\n",
    "    with open(json_file, \"r\") as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    records = []\n",
    "    for ts_str, (x_window, y_val) in data.items():\n",
    "        dt = datetime.fromisoformat(ts_str)  # Convert string to datetime\n",
    "        records.append((dt, x_window, y_val))\n",
    "    \n",
    "    # Sort by datetime\n",
    "    records.sort(key=lambda r: r[0])\n",
    "    return records\n",
    "\n",
    "def prepare_splits(records, train_size=30000, val_size=10000, test_size=15000):\n",
    "    \"\"\"\n",
    "    Splits the sorted records into train, val, and test sets (chronologically).\n",
    "    \n",
    "    Returns (X_train, y_train, X_val, y_val, X_test, y_test) as NumPy arrays.\n",
    "    \"\"\"\n",
    "    total_needed = train_size + val_size + test_size\n",
    "    subset = records[:total_needed]  # in case you have more data\n",
    "    \n",
    "    X_all = []\n",
    "    y_all = []\n",
    "    \n",
    "    for _, x_win, y_val in subset:\n",
    "        X_all.append(x_win)\n",
    "        y_all.append(y_val)\n",
    "    \n",
    "    X_all = np.array(X_all, dtype=np.float32)\n",
    "    y_all = np.array(y_all, dtype=np.float32)\n",
    "    \n",
    "    X_train = X_all[:train_size]\n",
    "    y_train = y_all[:train_size]\n",
    "    \n",
    "    X_val = X_all[train_size : train_size+val_size]\n",
    "    y_val = y_all[train_size : train_size+val_size]\n",
    "    \n",
    "    X_test = X_all[train_size+val_size : train_size+val_size+test_size]\n",
    "    y_test = y_all[train_size+val_size : train_size+val_size+test_size]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "# Load data from JSON\n",
    "all_records = load_data_from_json(\"BTC_train_data_new.json\")\n",
    "print(\"Total samples loaded:\", len(all_records))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (140000, 100) (140000,)\n",
      "Val shape:   (20000, 100) (20000,)\n",
      "Test shape:  (20000, 100) (20000,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split into train/val/test\n",
    "X_train_np, y_train_np, X_val_np, y_val_np, X_test_np, y_test_np = prepare_splits(\n",
    "    all_records,\n",
    "    train_size=140000,\n",
    "    val_size=20000,\n",
    "    test_size=20000\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train_np.shape, y_train_np.shape)\n",
    "print(\"Val shape:  \", X_val_np.shape,   y_val_np.shape)\n",
    "print(\"Test shape: \", X_test_np.shape,  y_test_np.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Return (features, target)\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = TimeSeriesDataset(X_train_np, y_train_np)\n",
    "val_dataset   = TimeSeriesDataset(X_val_np,   y_val_np)\n",
    "test_dataset  = TimeSeriesDataset(X_test_np,  y_test_np)\n",
    "\n",
    "# Create DataLoader objects\n",
    "#   - shuffle only for train\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "input_dim = X_train_np.shape[1]\n",
    "\n",
    "model = nn.Linear(in_features=input_dim, out_features=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/20] Train MSE: 18597.3799 | Val MSE: 58513.2072\n",
      "[Epoch 2/20] Train MSE: 17925.3028 | Val MSE: 74497.1344\n",
      "[Epoch 3/20] Train MSE: 17199.0672 | Val MSE: 64591.0666\n",
      "[Epoch 4/20] Train MSE: 17675.9906 | Val MSE: 56952.8802\n",
      "[Epoch 5/20] Train MSE: 17457.9061 | Val MSE: 57242.8312\n",
      "[Epoch 6/20] Train MSE: 17608.0719 | Val MSE: 56425.4542\n",
      "[Epoch 7/20] Train MSE: 17000.9513 | Val MSE: 62292.5403\n",
      "[Epoch 8/20] Train MSE: 17504.1561 | Val MSE: 58344.7456\n",
      "[Epoch 9/20] Train MSE: 17054.2809 | Val MSE: 65594.9181\n",
      "[Epoch 10/20] Train MSE: 17230.4352 | Val MSE: 55497.5701\n",
      "[Epoch 11/20] Train MSE: 16945.9657 | Val MSE: 55192.7850\n",
      "[Epoch 12/20] Train MSE: 17190.9933 | Val MSE: 54986.6254\n",
      "[Epoch 13/20] Train MSE: 17447.8068 | Val MSE: 57449.9099\n",
      "[Epoch 14/20] Train MSE: 16497.2236 | Val MSE: 60419.1852\n",
      "[Epoch 15/20] Train MSE: 16519.0755 | Val MSE: 56327.7269\n",
      "[Epoch 16/20] Train MSE: 16238.7007 | Val MSE: 74235.9477\n",
      "[Epoch 17/20] Train MSE: 16435.2349 | Val MSE: 60842.4948\n",
      "[Epoch 18/20] Train MSE: 16739.4963 | Val MSE: 57850.7697\n",
      "[Epoch 19/20] Train MSE: 16587.0928 | Val MSE: 63665.3231\n",
      "[Epoch 20/20] Train MSE: 16593.4486 | Val MSE: 56806.3198\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=1e-3, device=\"cpu\"):\n",
    "    model.to(device)  # Move model to the appropriate device\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # ---- TRAINING PHASE ----\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)  # Move to GPU if available\n",
    "            \n",
    "            # Forward pass\n",
    "            pred = model(batch_x).squeeze(1)  # shape (batch_size)\n",
    "            loss = F.mse_loss(pred, batch_y)\n",
    "            \n",
    "            # Backprop\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        train_loss_mean = np.mean(train_losses)\n",
    "        \n",
    "        # ---- VALIDATION PHASE ----\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "                pred = model(batch_x).squeeze(1)\n",
    "                loss = F.mse_loss(pred, batch_y)\n",
    "                val_losses.append(loss.item())\n",
    "        \n",
    "        val_loss_mean = np.mean(val_losses)\n",
    "        \n",
    "        print(f\"[Epoch {epoch}/{epochs}] \"\n",
    "              f\"Train MSE: {train_loss_mean:.4f} | \"\n",
    "              f\"Val MSE: {val_loss_mean:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "train_model(model, train_loader, val_loader, epochs=20, lr=2e-4 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
